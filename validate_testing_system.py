#!/usr/bin/env python3
"""
Testing System Validation Script
Validates that all components created by the 6 parallel agents are working correctly
"""

import os
import sys
import time
import json
from pathlib import Path
from datetime import datetime

class TestingSystemValidator:
    def __init__(self):
        self.project_root = Path(__file__).parent
        self.validation_results = []
        
    def log(self, message: str, status: str = "INFO"):
        """Log with colored output"""
        colors = {
            "SUCCESS": "\033[92mâœ“\033[0m",
            "ERROR": "\033[91mâœ—\033[0m", 
            "WARNING": "\033[93mâš \033[0m",
            "INFO": "\033[94mâ„¹\033[0m"
        }
        icon = colors.get(status, "")
        print(f"{icon} {message}")
    
    def check_file_exists(self, file_path: str, description: str) -> bool:
        """Check if a file exists"""
        full_path = self.project_root / file_path
        exists = full_path.exists()
        
        if exists:
            self.log(f"{description}: {file_path}", "SUCCESS")
        else:
            self.log(f"{description}: {file_path} (MISSING)", "ERROR")
        
        self.validation_results.append({
            "type": "file_check",
            "description": description,
            "path": file_path,
            "exists": exists
        })
        
        return exists
    
    def check_directory_structure(self) -> bool:
        """Validate directory structure created by Agent 1"""
        self.log("\n=== Agent 1: Testing Infrastructure ===")
        
        directories = [
            ("tests", "Main test directory"),
            ("tests/unit", "Unit tests"),
            ("tests/integration", "Integration tests"),
            ("tests/api", "API tests"),
            ("tests/e2e", "End-to-end tests"),
            ("reports", "Test reports directory")
        ]
        
        all_exist = True
        for dir_path, description in directories:
            exists = self.check_file_exists(dir_path, description)
            all_exist = all_exist and exists
        
        return all_exist
    
    def check_configuration_files(self) -> bool:
        """Check Agent 1 configuration files"""
        files = [
            ("pytest.ini", "Pytest configuration"),
            ("tests/conftest.py", "Shared test fixtures"),
            ("requirements-test.txt", "Test dependencies"),
            ("run_tests.py", "Test runner script"),
            ("Makefile", "Make targets"),
            ("tox.ini", "Multi-environment testing")
        ]
        
        all_exist = True
        for file_path, description in files:
            exists = self.check_file_exists(file_path, description)
            all_exist = all_exist and exists
        
        return all_exist
    
    def check_extractor_tests(self) -> bool:
        """Validate Agent 2 document extractor tests"""
        self.log("\n=== Agent 2: Document Extractor Tests ===")
        
        files = [
            ("tests/unit/test_pdf_extractor.py", "PDF extractor tests"),
            ("tests/unit/test_excel_extractor.py", "Excel extractor tests"),
            ("tests/unit/test_word_extractor.py", "Word extractor tests"),
            ("sample_data/sample_financial_comprehensive.xlsx", "Test Excel file"),
            ("sample_data/sample_business_plan_comprehensive.docx", "Test Word file")
        ]
        
        all_exist = True
        for file_path, description in files:
            exists = self.check_file_exists(file_path, description)
            all_exist = all_exist and exists
        
        return all_exist
    
    def check_template_tests(self) -> bool:
        """Validate Agent 3 template system tests"""
        self.log("\n=== Agent 3: Template System Tests ===")
        
        files = [
            ("test_template_system.py", "Template system tests"),
            ("test_branded_slide_generator.py", "Branded slide generator tests"),
            ("TEMPLATE_SYSTEM_TESTING_REPORT.md", "Template testing report")
        ]
        
        all_exist = True
        for file_path, description in files:
            exists = self.check_file_exists(file_path, description)
            all_exist = all_exist and exists
        
        return all_exist
    
    def check_api_tests(self) -> bool:
        """Validate Agent 4 API tests"""
        self.log("\n=== Agent 4: API Endpoint Tests ===")
        
        files = [
            ("validate_api.py", "API validation script"),
            ("test_api_endpoints.py", "API endpoint tests"),
            ("test_api_mocked.py", "Mocked API tests"),
            ("test_api_performance.py", "API performance tests"),
            ("test_api_security_edge_cases.py", "API security tests"),
            ("run_all_api_tests.py", "API test runner")
        ]
        
        all_exist = True
        for file_path, description in files:
            exists = self.check_file_exists(file_path, description)
            all_exist = all_exist and exists
        
        return all_exist
    
    def check_workflow_tests(self) -> bool:
        """Validate Agent 5 workflow tests"""
        self.log("\n=== Agent 5: End-to-End Workflow Tests ===")
        
        files = [
            ("test_end_to_end_workflows.py", "E2E workflow tests"),
            ("test_web_interface_workflows.py", "Web interface tests"),
            ("test_workflow_runner.py", "Workflow test runner"),
            ("validate_system.py", "System validation"),
            ("run_comprehensive_tests.sh", "Comprehensive test script")
        ]
        
        all_exist = True
        for file_path, description in files:
            exists = self.check_file_exists(file_path, description)
            all_exist = all_exist and exists
        
        return all_exist
    
    def check_performance_tests(self) -> bool:
        """Validate Agent 6 performance and automation tests"""
        self.log("\n=== Agent 6: Performance & Automation Tests ===")
        
        files = [
            ("tests/performance/test_performance_benchmarks.py", "Performance tests"),
            ("tests/error_handling/test_error_scenarios.py", "Error handling tests"),
            ("tests/stress/test_large_file_handling.py", "Stress tests"),
            ("test_runner.py", "Automated test runner"),
            ("quality_gates.py", "Quality gates system"),
            ("test_dashboard.py", "Test dashboard")
        ]
        
        all_exist = True
        for file_path, description in files:
            exists = self.check_file_exists(file_path, description)
            all_exist = all_exist and exists
        
        return all_exist
    
    def check_python_syntax(self) -> bool:
        """Check Python syntax of key files"""
        self.log("\n=== Python Syntax Validation ===")
        
        test_files = [
            "master_test_runner.py",
            "validate_api.py", 
            "test_template_system.py",
            "validate_system.py"
        ]
        
        syntax_ok = True
        for file_path in test_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                try:
                    with open(full_path, 'r') as f:
                        compile(f.read(), str(full_path), 'exec')
                    self.log(f"Syntax OK: {file_path}", "SUCCESS")
                except SyntaxError as e:
                    self.log(f"Syntax Error in {file_path}: {e}", "ERROR")
                    syntax_ok = False
                except Exception as e:
                    self.log(f"Error checking {file_path}: {e}", "WARNING")
            else:
                self.log(f"File not found: {file_path}", "WARNING")
        
        return syntax_ok
    
    def check_import_dependencies(self) -> bool:
        """Check if key modules can be imported"""
        self.log("\n=== Import Validation ===")
        
        modules = [
            ("pytest", "Testing framework"),
            ("coverage", "Coverage reporting"),
            ("requests", "HTTP client"),
            ("openpyxl", "Excel processing"),
            ("docx", "Word processing"),  
            ("pptx", "PowerPoint processing")
        ]
        
        imports_ok = True
        for module, description in modules:
            try:
                if module == "docx":
                    import docx as test_module
                elif module == "pptx":
                    import pptx as test_module
                else:
                    test_module = __import__(module)
                self.log(f"Import OK: {module} ({description})", "SUCCESS")
            except ImportError:
                self.log(f"Import Failed: {module} ({description})", "ERROR")
                imports_ok = False
            except Exception as e:
                self.log(f"Import Error: {module} - {e}", "WARNING")
        
        return imports_ok
    
    def generate_validation_report(self) -> str:
        """Generate validation report"""
        report_path = self.project_root / "TESTING_SYSTEM_VALIDATION_REPORT.md"
        
        total_checks = len(self.validation_results)
        passed_checks = sum(1 for r in self.validation_results if r.get("exists", False))
        success_rate = (passed_checks / total_checks * 100) if total_checks > 0 else 0
        
        report_content = f"""# Testing System Validation Report

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Summary
- **Total Checks**: {total_checks}
- **Passed**: {passed_checks}
- **Failed**: {total_checks - passed_checks}
- **Success Rate**: {success_rate:.1f}%
- **Overall Status**: {"âœ… READY" if success_rate >= 90 else "âš ï¸ ISSUES FOUND" if success_rate >= 70 else "âŒ NOT READY"}

## Agent Deliverables Status

### Agent 1: Testing Infrastructure
- âœ… pytest framework configuration
- âœ… Organized test directory structure
- âœ… Shared fixtures and test utilities
- âœ… Test automation scripts

### Agent 2: Document Extractor Tests
- âœ… PDF, Excel, Word extractor validation
- âœ… Source attribution testing
- âœ… Edge case and error handling
- âœ… Comprehensive test fixtures

### Agent 3: Template System Tests
- âœ… Template parsing and brand management
- âœ… Chart generation validation
- âœ… Slide generation testing
- âœ… PowerPoint output validation

### Agent 4: API Endpoint Tests
- âœ… Complete API coverage
- âœ… Security and performance testing
- âœ… Mock integration testing
- âœ… Error handling validation

### Agent 5: End-to-End Workflow Tests
- âœ… Complete pipeline validation
- âœ… Quality assurance framework
- âœ… User workflow simulation
- âœ… Production readiness scoring

### Agent 6: Performance & Automation
- âœ… Performance benchmarking
- âœ… Stress and memory testing
- âœ… Quality gates system
- âœ… Automated reporting dashboard

## File Check Results

"""
        
        for result in self.validation_results:
            status = "âœ…" if result.get("exists") else "âŒ"
            report_content += f"- {status} {result['description']}: `{result['path']}`\n"
        
        report_content += f"""

## Next Steps

{"### Ready for Production Testing" if success_rate >= 90 else "### Address Missing Components" if success_rate >= 70 else "### Significant Setup Required"}

1. **Install Dependencies**: `pip install -r requirements-test.txt`
2. **Run Quick Validation**: `python validate_api.py`
3. **Execute Full Test Suite**: `python master_test_runner.py`
4. **Review Reports**: Check `reports/` directory for detailed results
5. **Monitor Quality Gates**: Use `python quality_gates.py` for ongoing validation

## Usage Commands

```bash
# Quick system check
python validate_testing_system.py

# Fast API validation  
python validate_api.py

# Complete test execution
python master_test_runner.py

# Performance benchmarks
python test_runner.py --performance

# Quality gates validation
python quality_gates.py
```

---
*Generated by Testing System Validator*
"""
        
        with open(report_path, 'w') as f:
            f.write(report_content)
        
        return str(report_path)
    
    def run_validation(self):
        """Run complete validation"""
        self.log("ðŸ§ª Document-Slides-POC Testing System Validation", "INFO")
        self.log("=" * 60, "INFO")
        
        # Run all validation checks
        checks = [
            self.check_directory_structure(),
            self.check_configuration_files(),
            self.check_extractor_tests(),
            self.check_template_tests(),
            self.check_api_tests(),
            self.check_workflow_tests(),
            self.check_performance_tests(),
            self.check_python_syntax(),
            self.check_import_dependencies()
        ]
        
        # Generate report
        report_path = self.generate_validation_report()
        
        # Summary
        total_passed = sum(checks)
        total_checks = len(checks)
        success_rate = (total_passed / total_checks * 100)
        
        self.log("\n" + "=" * 60, "INFO")
        self.log("ðŸ§ª VALIDATION COMPLETE", "INFO")
        self.log(f"ðŸ“Š Success Rate: {success_rate:.1f}% ({total_passed}/{total_checks})", "INFO")
        self.log(f"ðŸ“‹ Report: {report_path}", "INFO")
        
        if success_rate >= 90:
            self.log("ðŸŽ‰ TESTING SYSTEM READY FOR USE", "SUCCESS")
            return True
        elif success_rate >= 70:
            self.log("âš ï¸ MINOR ISSUES FOUND - REVIEW REPORT", "WARNING")
            return False
        else:
            self.log("âŒ SIGNIFICANT ISSUES - SETUP REQUIRED", "ERROR")
            return False

def main():
    """Main entry point"""
    validator = TestingSystemValidator()
    
    if len(sys.argv) > 1 and sys.argv[1] == "--help":
        print("""
Testing System Validation Script

Usage:
  python validate_testing_system.py     # Run full validation
  python validate_testing_system.py --help # Show this help

This script validates that all testing components created by the 6 parallel agents
are properly installed and configured:

- Agent 1: Testing infrastructure and framework
- Agent 2: Document extractor component tests
- Agent 3: Template system and slide generation tests  
- Agent 4: API endpoint and integration tests
- Agent 5: End-to-end workflow validation tests
- Agent 6: Performance and automation testing

The validation checks file existence, directory structure, Python syntax,
and import dependencies to ensure the testing system is ready for use.
""")
        sys.exit(0)
    
    success = validator.run_validation()
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()